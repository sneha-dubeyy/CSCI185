{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d429b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sneha Dubey (W1618643)\n",
    "# Dr. Chen\n",
    "# CSCI 185\n",
    "# 27 February 2024\n",
    "# Homework 3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff9ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/snehadubey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/snehadubey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb0ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents/d1.txt has 30086 tokens\n",
      "Documents/d2.txt has 33357 tokens\n",
      "Documents/d3.txt has 28843 tokens\n",
      "Documents/d4.txt has 36442 tokens\n",
      "Documents/d5.txt has 37060 tokens\n",
      "Documents/d6.txt has 39649 tokens\n",
      "Documents/d7.txt has 25893 tokens\n",
      "Documents/d8.txt has 26603 tokens\n",
      "Documents/d9.txt has 17864 tokens\n",
      "Documents/d10.txt has 22683 tokens\n",
      "Total Tokens Generated:  298480  tokens\n"
     ]
    }
   ],
   "source": [
    "#Question 1, Part A\n",
    "\n",
    "totalTokenCount = 0\n",
    "\n",
    "for i in range (1, 11):\n",
    "    with open(\"Documents/d\" + str(i) + \".txt\", 'r') as currDoc:\n",
    "        contents = currDoc.read()\n",
    "    currTokens = word_tokenize(contents)\n",
    "    currTokenCount = len(currTokens)\n",
    "    totalTokenCount += currTokenCount\n",
    "    print(\"Documents/d\" + str(i) + \".txt\" + \" has \" + str(currTokenCount) + \" tokens\")\n",
    "\n",
    "print(\"Total Tokens Generated: \", str(totalTokenCount), \" tokens\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df15f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Tokens Generated:  16750  tokens\n"
     ]
    }
   ],
   "source": [
    "#Question 1, Part B\n",
    "\n",
    "allText = \"\"\n",
    "for i in range (1, 11):\n",
    "    with open(\"Documents/d\" + str(i) + \".txt\", 'r') as currDoc:\n",
    "        allText += currDoc.read()\n",
    "\n",
    "allTokens = word_tokenize(allText)\n",
    "\n",
    "uniqueTokens = []\n",
    "\n",
    "for token in allTokens:\n",
    "    if token not in uniqueTokens:\n",
    "        uniqueTokens.append(token)\n",
    "    \n",
    "print(\"Total Unique Tokens Generated: \", str(len(uniqueTokens)), \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857ae2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents/d1.txt has 17760 tokens after removing stop words\n",
      "Documents/d2.txt has 21166 tokens after removing stop words\n",
      "Documents/d3.txt has 17071 tokens after removing stop words\n",
      "Documents/d4.txt has 22045 tokens after removing stop words\n",
      "Documents/d5.txt has 22615 tokens after removing stop words\n",
      "Documents/d6.txt has 23879 tokens after removing stop words\n",
      "Documents/d7.txt has 15902 tokens after removing stop words\n",
      "Documents/d8.txt has 16416 tokens after removing stop words\n",
      "Documents/d9.txt has 11225 tokens after removing stop words\n",
      "Documents/d10.txt has 14317 tokens after removing stop words\n",
      "Total Tokens Generated After Removing Stop Words:  182396  tokens\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part A\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "totalTokenCount = 0\n",
    "\n",
    "for i in range (1, 11):\n",
    "    with open(\"Documents/d\" + str(i) + \".txt\", 'r') as currDoc:\n",
    "        contents = currDoc.read()\n",
    "    currTokens = word_tokenize(contents)\n",
    "    filteredTokens = [w for w in currTokens if not w.lower() in stopWords]\n",
    "    currTokenCount = len(filteredTokens)\n",
    "    totalTokenCount += currTokenCount\n",
    "    print(\"Documents/d\" + str(i) + \".txt\" + \" has \" + str(currTokenCount) + \" tokens after removing stop words\")\n",
    "\n",
    "print(\"Total Tokens Generated After Removing Stop Words: \", str(totalTokenCount), \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f038faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Tokens Generated After Removing Stop Words:  16488  tokens\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part B\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "allText = \"\"\n",
    "for i in range (1, 11):\n",
    "    with open(\"Documents/d\" + str(i) + \".txt\", 'r') as currDoc:\n",
    "        allText += currDoc.read()\n",
    "\n",
    "allTokens = word_tokenize(allText)\n",
    "filteredTokens = [w for w in allTokens if not w.lower() in stopWords]\n",
    "uniqueTokens = []\n",
    "\n",
    "for token in filteredTokens:\n",
    "    if token not in uniqueTokens:\n",
    "        uniqueTokens.append(token)\n",
    "    \n",
    "print(\"Total Unique Tokens Generated After Removing Stop Words: \", str(len(uniqueTokens)), \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d66daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " ['10' '11' '12' ... 'zephyrs' 'zone' 'zounds']\n",
      "\n",
      "\n",
      "TF-IDF Matrix: \n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00207463 0.00181506 0.00244048 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.00149544]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "\n",
    "documents = []\n",
    "for i in range (1, 11):\n",
    "    with open(\"Documents/d\" + str(i) + \".txt\", 'r') as currDoc:\n",
    "        documents.append(currDoc.read())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary: \\n\", feature_names)\n",
    "print(\"\\n\")\n",
    "print(\"TF-IDF Matrix: \\n\", tfidf_matrix.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a3bf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Between Documents: \n",
      " [[1.         0.72946613 0.73583784 0.770259   0.79669928 0.75791424\n",
      "  0.72230862 0.79729685 0.78409789 0.70623022]\n",
      " [0.72946613 1.         0.66525956 0.7126399  0.73046881 0.68667594\n",
      "  0.75745155 0.7328169  0.73437245 0.6601013 ]\n",
      " [0.73583784 0.66525956 1.         0.70838306 0.73164738 0.68733364\n",
      "  0.67336767 0.72624597 0.71419834 0.64581577]\n",
      " [0.770259   0.7126399  0.70838306 1.         0.76921663 0.72303565\n",
      "  0.73589728 0.76299183 0.75453351 0.69559487]\n",
      " [0.79669928 0.73046881 0.73164738 0.76921663 1.         0.74813174\n",
      "  0.72130775 0.7958372  0.80150793 0.70901435]\n",
      " [0.75791424 0.68667594 0.68733364 0.72303565 0.74813174 1.\n",
      "  0.68543967 0.76985797 0.75193148 0.67804681]\n",
      " [0.72230862 0.75745155 0.67336767 0.73589728 0.72130775 0.68543967\n",
      "  1.         0.72918983 0.71243264 0.65401409]\n",
      " [0.79729685 0.7328169  0.72624597 0.76299183 0.7958372  0.76985797\n",
      "  0.72918983 1.         0.83281273 0.73218329]\n",
      " [0.78409789 0.73437245 0.71419834 0.75453351 0.80150793 0.75193148\n",
      "  0.71243264 0.83281273 1.         0.73161253]\n",
      " [0.70623022 0.6601013  0.64581577 0.69559487 0.70901435 0.67804681\n",
      "  0.65401409 0.73218329 0.73161253 1.        ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Between Documents: \\n\", cosine_similarities)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
